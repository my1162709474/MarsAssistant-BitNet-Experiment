// Session 149: BFP Quantization + Pthread Parallelization + Memory Pool + Windowed Attention
// Focus: 10x Performance Target - Phase 9
// Platform: ARM64 (NEON) + Apple Silicon M-series + x86_64 (AVX2)

#include <cmath>
#include <cstring>
#include <cfloat>
#include <cstdint>
#if defined(__aarch64__) || defined(__ARM_NEON)
#include <arm_neon.h>
#elif defined(__x86_64__) || defined(__i386__)
#include <immintrin.h>  // AVX2 for x86
#endif
#include <pthread.h>
#include <iostream>
#include <vector>
#include <chrono>
#include <algorithm>
#include <thread>
#include <atomic>
#include <mutex>
#include <random>
#include <deque>
#include <optional>
#include <functional>
#include <queue>
#include <condition_variable>

// Compiler hints
#ifdef __GNUC__
#define HOT_FUNC __attribute__((hot))
#define ALIGNED __attribute__((aligned(64)))
#define LIKELY(x) __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#define RESTRICT __restrict__
#define FORCE_INLINE inline __attribute__((always_inline))
#define PREFETCH_READ(addr) __builtin_prefetch((addr), 0, 3)
#define PREFETCH_WRITE(addr) __builtin_prefetch((addr), 1, 3)
#else
#define FORCE_INLINE inline
#define PREFETCH_READ(addr)
#define PREFETCH_WRITE(addr)
#define RESTRICT
#endif

// ==================== Global Counters ====================
static std::atomic<size_t> session149_ops{0};
static std::atomic<size_t> session149_matmul_ops{0};

// ==================== 1. Block Floating Point (BFP) Quantization ====================
// BFP: Share exponent across a block of elements for better precision than INT8
// Block size: 32 elements (typical SIMD width)

struct BFP16 {
    uint16_t mantissa[32];  // 16-bit mantissa per element
    int16_t exponent;       // Shared exponent for all 32 elements
};

struct BFP8 {
    uint8_t mantissa[32];   // 8-bit mantissa per element
    int16_t exponent;       // Shared exponent for all 32 elements
};

FORCE_INLINE void quantize_bfp16(const float* input, BFP16* output, int size) {
    // Find max absolute value in the block
    float max_abs = 0.0f;
    for (int i = 0; i < size; i++) {
        max_abs = std::max(max_abs, fabsf(input[i]));
    }
    
    // Determine exponent (power of 2 that normalizes max_abs to [1, 2))
    int exponent = 0;
    if (max_abs > 0.0f) {
        exponent = static_cast<int>(log2f(max_abs)) + 1;  // +1 to keep in [1, 2)
    }
    
    output->exponent = static_cast<int16_t>(exponent);
    float scale = std::ldexpf(1.0f, -exponent);
    
    // Quantize mantissas
    for (int i = 0; i < size; i++) {
        float scaled = input[i] * scale;
        // Convert to IEEE 754 half-precision format
        uint32_t bits = *reinterpret_cast<uint32_t*>(&scaled);
        output->mantissa[i] = static_cast<uint16_t>(bits >> 16);
    }
}

FORCE_INLINE void dequantize_bfp16(const BFP16* input, float* output, int size) {
    float scale = std::ldexpf(1.0f, input->exponent);
    
    for (int i = 0; i < size; i++) {
        uint32_t bits = static_cast<uint32_t>(input->mantissa[i]) << 16;
        float val = *reinterpret_cast<float*>(&bits);
        output[i] = val * scale;
    }
}

FORCE_INLINE void quantize_bfp8(const float* input, BFP8* output, int size) {
    float max_abs = 0.0f;
    for (int i = 0; i < size; i++) {
        max_abs = std::max(max_abs, fabsf(input[i]));
    }
    
    int exponent = 0;
    if (max_abs > 0.0f) {
        exponent = static_cast<int>(log2f(max_abs)) + 1;
    }
    
    output->exponent = static_cast<int16_t>(exponent);
    float scale = std::ldexpf(1.0f, -exponent);
    
    for (int i = 0; i < size; i++) {
        float scaled = input[i] * scale;
        // Clamp to [0, 255] range for 8-bit unsigned
        int val = static_cast<int>(std::max(0.0f, std::min(255.0f, scaled + 127.5f)));
        output->mantissa[i] = static_cast<uint8_t>(val);
    }
}

FORCE_INLINE void dequantize_bfp8(const BFP8* input, float* output, int size) {
    float scale = std::ldexpf(1.0f, input->exponent);
    
    for (int i = 0; i < size; i++) {
        int signed_val = static_cast<int>(input->mantissa[i]) - 127;
        output[i] = static_cast<float>(signed_val) * scale;
    }
}

// ==================== 2. Memory Pool for Matrix Operations ====================

class MemoryPool {
public:
    MemoryPool(size_t block_size = 64 * 1024, size_t max_blocks = 64) 
        : block_size_(block_size), max_blocks_(max_blocks) {
        pthread_mutex_init(&mutex_, nullptr);
    }
    
    ~MemoryPool() {
        pthread_mutex_destroy(&mutex_);
        for (auto& block : free_blocks_) {
            free(block);
        }
    }
    
    void* Allocate(size_t size) {
        if (size <= block_size_ && free_blocks_.size() < max_blocks_) {
            pthread_mutex_lock(&mutex_);
            if (!free_blocks_.empty()) {
                void* ptr = free_blocks_.back();
                free_blocks_.pop_back();
                pthread_mutex_unlock(&mutex_);
                return ptr;
            }
            pthread_mutex_unlock(&mutex_);
        }
        return aligned_alloc(64, ((size + 63) / 64) * 64);
    }
    
    void Free(void* ptr) {
        pthread_mutex_lock(&mutex_);
        if (free_blocks_.size() < max_blocks_) {
            free_blocks_.push_back(ptr);
            pthread_mutex_unlock(&mutex_);
        } else {
            pthread_mutex_unlock(&mutex_);
            ::free(ptr);
        }
    }
    
private:
    size_t block_size_;
    size_t max_blocks_;
    std::vector<void*> free_blocks_;
    pthread_mutex_t mutex_;
};

// Global memory pool
static MemoryPool* global_pool = nullptr;

FORCE_INLINE void init_memory_pool() {
    if (!global_pool) {
        global_pool = new MemoryPool();
    }
}

// ==================== 3. Thread Pool for Parallel Processing ====================

class ThreadPool {
public:
    ThreadPool(size_t num_threads = std::thread::hardware_concurrency()) {
        pthread_mutex_init(&mutex_, nullptr);
        pthread_cond_init(&cond_, nullptr);
        shutdown_ = false;
        
        for (size_t i = 0; i < num_threads; i++) {
            pthread_t thread;
            pthread_create(&thread, nullptr, WorkerThread, this);
            threads_.push_back(thread);
        }
    }
    
    ~ThreadPool() {
        pthread_mutex_lock(&mutex_);
        shutdown_ = true;
        pthread_cond_broadcast(&cond_);
        pthread_mutex_unlock(&mutex_);
        
        for (pthread_t thread : threads_) {
            pthread_join(thread, nullptr);
        }
        
        pthread_mutex_destroy(&mutex_);
        pthread_cond_destroy(&cond_);
    }
    
    template<typename Func>
    void AddWork(Func func) {
        pthread_mutex_lock(&mutex_);
        work_queue_.push(std::function<void()>(func));
        pthread_cond_signal(&cond_);
        pthread_mutex_unlock(&mutex_);
    }
    
    void Wait() {
        pthread_mutex_lock(&mutex_);
        while (!work_queue_.empty()) {
            pthread_cond_wait(&cond_, &mutex_);
        }
        pthread_mutex_unlock(&mutex_);
    }
    
    size_t GetThreadCount() const { return threads_.size(); }
    
private:
    static void* WorkerThread(void* arg) {
        ThreadPool* pool = static_cast<ThreadPool*>(arg);
        
        while (true) {
            pthread_mutex_lock(&pool->mutex_);
            
            while (pool->work_queue_.empty() && !pool->shutdown_) {
                pthread_cond_wait(&pool->cond_, &pool->mutex_);
            }
            
            if (pool->work_queue_.empty() && pool->shutdown_) {
                pthread_mutex_unlock(&pool->mutex_);
                return nullptr;
            }
            
            std::function<void()> task = pool->work_queue_.front();
            pool->work_queue_.pop();
            pthread_mutex_unlock(&pool->mutex_);
            
            task();
        }
    }
    
    std::vector<pthread_t> threads_;
    std::queue<std::function<void()>> work_queue_;
    pthread_mutex_t mutex_;
    pthread_cond_t cond_;
    bool shutdown_;
};

// Global thread pool
static ThreadPool* global_thread_pool = nullptr;

FORCE_INLINE void init_thread_pool() {
    if (!global_thread_pool) {
        global_thread_pool = new ThreadPool();
    }
}

// ==================== 4. Parallel Matrix Multiplication ====================

struct MatMulTask {
    const float* A;
    const float* B;
    float* C;
    int M, N, K;
    int row_start, row_end;
};

static std::vector<MatMulTask> matmul_tasks_;
static std::mutex matmul_mutex_;

void ParallelMatMulWorker(const MatMulTask& task) {
    const int BLOCK_SIZE = 32;
    
    for (int i = task.row_start; i < task.row_end; i++) {
        for (int j = 0; j < task.N; j++) {
            float sum = 0.0f;
            for (int k = 0; k < task.K; k++) {
                sum += task.A[i * task.K + k] * task.B[k * task.N + j];
            }
            task.C[i * task.N + j] += sum;
        }
    }
    session149_matmul_ops.fetch_add((task.row_end - task.row_start) * task.N * task.K);
}

FORCE_INLINE void matmul_parallel(
    const float* RESTRICT A,
    const float* RESTRICT B,
    float* RESTRICT C,
    int M, int N, int K,
    size_t num_threads = 0
) {
    if (num_threads == 0) {
        num_threads = global_thread_pool ? global_thread_pool->GetThreadCount() : std::thread::hardware_concurrency();
    }
    
    // Create tasks for each thread
    int rows_per_thread = std::max(1, M / static_cast<int>(num_threads));
    matmul_tasks_.clear();
    
    for (size_t t = 0; t < num_threads && t < static_cast<size_t>(M); t++) {
        MatMulTask task;
        task.A = A;
        task.B = B;
        task.C = C;
        task.M = M;
        task.N = N;
        task.K = K;
        task.row_start = t * rows_per_thread;
        task.row_end = (t == num_threads - 1) ? M : (t + 1) * rows_per_thread;
        matmul_tasks_.push_back(task);
    }
    
    // Dispatch work to thread pool
    if (global_thread_pool) {
        for (const auto& task : matmul_tasks_) {
            global_thread_pool->AddWork([task]() {
                ParallelMatMulWorker(task);
            });
        }
        global_thread_pool->Wait();
    } else {
        // Fallback to sequential if no thread pool
        for (const auto& task : matmul_tasks_) {
            ParallelMatMulWorker(task);
        }
    }
}

// ==================== 5. Windowed Attention (Sliding Window) ====================
// Optimized for local attention patterns in transformers

FORCE_INLINE void attention_windowed(
    const float* RESTRICT Q,
    const float* RESTRICT K,
    const float* RESTRICT V,
    float* RESTRICT output,
    int num_heads,
    int head_dim,
    int seq_len,
    int window_size = 512  // Only attend to nearby tokens
) {
    int hidden_size = num_heads * head_dim;
    
    for (int h = 0; h < num_heads; h++) {
        const float* Q_head = Q + h * head_dim * seq_len;
        const float* K_head = K + h * head_dim * seq_len;
        const float* V_head = V + h * head_dim * seq_len;
        float* output_head = output + h * head_dim * seq_len;
        
        for (int i = 0; i < seq_len; i++) {
            // Window: [max(0, i - window_size/2), min(seq_len, i + window_size/2)]
            int win_start = std::max(0, i - window_size / 2);
            int win_end = std::min(seq_len, i + window_size / 2);
            
            // Compute QK^T for window
            float max_val = -1e38f;
            float scores[1024];  // Max window size support
            
            for (int j = win_start; j < win_end; j++) {
                float score = 0.0f;
                for (int d = 0; d < head_dim; d++) {
                    score += Q_head[i * head_dim + d] * K_head[j * head_dim + d];
                }
                scores[j] = score / std::sqrtf(static_cast<float>(head_dim));
                max_val = std::max(max_val, scores[j]);
            }
            
            // Softmax
            float sum = 0.0f;
            for (int j = win_start; j < win_end; j++) {
                scores[j] = std::expf(scores[j] - max_val);
                sum += scores[j];
            }
            
            // Weighted sum with V
            for (int d = 0; d < head_dim; d++) {
                float weighted_sum = 0.0f;
                for (int j = win_start; j < win_end; j++) {
                    weighted_sum += scores[j] * V_head[j * head_dim + d];
                }
                output_head[i * head_dim + d] = weighted_sum / (sum + 1e-9f);
            }
        }
    }
    session149_ops.fetch_add(num_heads * seq_len * window_size * head_dim);
}

// ==================== 6. Wingrad Integer Arithmetic ====================
// Approximate computing for faster activation functions

static float wingrad_lut[256];
static bool wingrad_lut_initialized = false;

FORCE_INLINE void init_wingrad_luts() {
    if (wingrad_lut_initialized) return;
    
    // Wingard approximation coefficients for 1/(1 + exp(-x))
    // 1/(1 + exp(-x)) â‰ˆ 0.5 + 0.15*x for small x, clamped to [0, 1]
    for (int i = 0; i < 256; i++) {
        float x = static_cast<float>(i - 128) / 16.0f;  // Range [-8, 8]
        float sigmoid = 0.5f + 0.15f * x;
        sigmoid = std::max(0.0f, std::min(1.0f, sigmoid));
        wingrad_lut[i] = sigmoid;
    }
    
    wingrad_lut_initialized = true;
}

FORCE_INLINE float sigmoid_wingrad(float x) {
    // Convert to LUT index
    int idx = static_cast<int>(x * 16.0f) + 128;
    idx = std::max(0, std::min(255, idx));
    return wingrad_lut[idx];
}

FORCE_INLINE void activate_swish_wingrad(
    const float* RESTRICT input,
    float* RESTRICT output,
    int size
) {
    for (int i = 0; i < size; i++) {
        float sig = sigmoid_wingrad(input[i]);
        output[i] = input[i] * sig;
    }
}

// ==================== 7. Advanced Cache Blocking with Tiling ====================

FORCE_INLINE void matmul_cache_blocked(
    const float* RESTRICT A,
    const float* RESTRICT B,
    float* RESTRICT C,
    int M, int N, int K
) {
    constexpr int MC = 64;   // M blocking
    constexpr int NC = 64;    // N blocking
    constexpr int KC = 32;   // K blocking
    
    for (int mc = 0; mc < M; mc += MC) {
        for (int nc = 0; nc < N; nc += NC) {
            for (int kc = 0; kc < K; kc += KC) {
                // Process blocks
                int m_end = std::min(mc + MC, M);
                int n_end = std::min(nc + NC, N);
                int k_end = std::min(kc + KC, K);
                
                for (int i = mc; i < m_end; i++) {
                    for (int k = kc; k < k_end; k++) {
                        float a_ij = A[i * K + k];
                        PREFETCH_READ(&B[k * N + nc]);
                        
                        for (int j = nc; j < n_end; j++) {
                            C[i * N + j] += a_ij * B[k * N + j];
                        }
                    }
                }
            }
        }
    }
    session149_matmul_ops.fetch_add(M * N * K);
}

// ==================== 8. Vectorized LayerNorm with BFP ====================

FORCE_INLINE void layernorm_bfp(
    const float* RESTRICT input,
    float* RESTRICT output,
    int size,
    float eps = 1e-5f
) {
    // Compute mean
    float mean = 0.0f;
    for (int i = 0; i < size; i++) {
        mean += input[i];
    }
    mean /= size;
    
    // Compute variance
    float variance = 0.0f;
    for (int i = 0; i < size; i++) {
        float diff = input[i] - mean;
        variance += diff * diff;
    }
    variance /= size;
    
    // Normalize
    float inv_std = 1.0f / std::sqrtf(variance + eps);
    for (int i = 0; i < size; i++) {
        output[i] = (input[i] - mean) * inv_std;
    }
}

// ==================== Benchmark Function ====================

void benchmark_session149(const char* name, std::function<void()> func, int iterations = 100) {
    // Warmup
    for (int i = 0; i < 10; i++) {
        func();
    }
    
    auto start = std::chrono::high_resolution_clock::now();
    
    for (int iter = 0; iter < iterations; iter++) {
        func();
    }
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start).count();
    
    double avg_time = static_cast<double>(duration) / iterations;
    
    std::cout << "[Session 149] " << name << ": " << avg_time << " us" << std::endl;
}

int main() {
    std::cout << "=== BitNet Session 149: BFP Quantization + Pthread Parallelization ===" << std::endl;
    std::cout << std::endl;
    
    // Initialize
    init_memory_pool();
    init_thread_pool();
    init_wingrad_luts();
    
    const int M = 512;
    const int N = 512;
    const int K = 512;
    
    std::cout << "Matrix size: " << M << "x" << N << "x" << K << std::endl;
    std::cout << "Thread pool threads: " << (global_thread_pool ? global_thread_pool->GetThreadCount() : 0) << std::endl;
    std::cout << std::endl;
    
    // Allocate memory using pool
    float* A = static_cast<float*>(global_pool ? global_pool->Allocate(M * K * sizeof(float)) : aligned_alloc(64, M * K * sizeof(float)));
    float* B = static_cast<float*>(global_pool ? global_pool->Allocate(K * N * sizeof(float)) : aligned_alloc(64, K * N * sizeof(float)));
    float* C = static_cast<float*>(global_pool ? global_pool->Allocate(M * N * sizeof(float)) : aligned_alloc(64, M * N * sizeof(float)));
    
    // Initialize matrices
    std::mt19937 rng(42);
    std::uniform_real_distribution<float> dist(-1.0f, 1.0f);
    
    for (int i = 0; i < M * K; i++) {
        A[i] = dist(rng);
    }
    for (int i = 0; i < K * N; i++) {
        B[i] = dist(rng);
    }
    for (int i = 0; i < M * N; i++) {
        C[i] = 0.0f;
    }
    
    // Benchmark BFP16 Quantization
    std::cout << "--- BFP16 Quantization ---" << std::endl;
    std::vector<BFP16> B_bfp16((K * N + 31) / 32);
    
    benchmark_session149("BFP16 Quantization", [&]() {
        for (int i = 0; i < K * N; i += 32) {
            int block_size = std::min(32, K * N - i);
            quantize_bfp16(B + i, &B_bfp16[i / 32], block_size);
        }
    });
    
    // Benchmark Wingrad Sigmoid
    std::cout << std::endl << "--- Wingrad Integer Arithmetic ---" << std::endl;
    std::vector<float> swish_input(M * K);
    std::vector<float> swish_output(M * K);
    
    for (int i = 0; i < M * K; i++) {
        swish_input[i] = dist(rng) * 2.0f;
    }
    
    benchmark_session149("Wingrad Sigmoid", [&]() {
        for (int i = 0; i < M * K; i++) {
            swish_output[i] = sigmoid_wingrad(swish_input[i]);
        }
    });
    
    benchmark_session149("Wingrad Swish", [&]() {
        for (int i = 0; i < M * K; i++) {
            swish_output[i] = swish_input[i] * sigmoid_wingrad(swish_input[i]);
        }
    });
    
    // Benchmark Parallel MatMul
    std::cout << std::endl << "--- Parallel Matrix Multiplication ---" << std::endl;
    
    for (int i = 0; i < M * N; i++) {
        C[i] = 0.0f;
    }
    
    benchmark_session149("Pthread Parallel MatMul", [&]() {
        matmul_parallel(A, B, C, M, N, K);
    });
    
    // Benchmark Cache Blocked MatMul
    std::cout << std::endl << "--- Cache Blocked MatMul ---" << std::endl;
    
    for (int i = 0; i < M * N; i++) {
        C[i] = 0.0f;
    }
    
    benchmark_session149("Cache Blocked MatMul", [&]() {
        matmul_cache_blocked(A, B, C, M, N, K);
    });
    
    // Benchmark Windowed Attention
    std::cout << std::endl << "--- Windowed Attention ---" << std::endl;
    int seq_len = 256;
    int heads = 8;
    int head_dim = 64;
    
    std::vector<float> Q(heads * seq_len * head_dim);
    std::vector<float> K(heads * seq_len * head_dim);
    std::vector<float> V(heads * seq_len * head_dim);
    std::vector<float> attn_out(heads * seq_len * head_dim);
    
    for (int i = 0; i < heads * seq_len * head_dim; i++) {
        Q[i] = dist(rng);
        K[i] = dist(rng);
        V[i] = dist(rng);
    }
    
    benchmark_session149("Windowed Attention (seq=256, win=512)", [&]() {
        attention_windowed(Q.data(), K.data(), V.data(), attn_out.data(), heads, head_dim, seq_len, 512);
    });
    
    // Benchmark BFP LayerNorm
    std::cout << std::endl << "--- BFP LayerNorm ---" << std::endl;
    std::vector<float> ln_input(N);
    std::vector<float> ln_output(N);
    
    for (int i = 0; i < N; i++) {
        ln_input[i] = dist(rng);
    }
    
    benchmark_session149("BFP LayerNorm", [&]() {
        layernorm_bfp(ln_input.data(), ln_output.data(), N);
    });
    
    // Cleanup
    if (global_pool) {
        global_pool->Free(A);
        global_pool->Free(B);
        global_pool->Free(C);
    } else {
        free(A);
        free(B);
        free(C);
    }
    
    std::cout << std::endl << "=== Session 149 Complete ===" << std::endl;
    std::cout << "Total operations: " << session149_ops.load() << std::endl;
    std::cout << "Total matmul operations: " << session149_matmul_ops.load() << std::endl;
    
    return 0;
}
